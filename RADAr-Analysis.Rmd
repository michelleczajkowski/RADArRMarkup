---
title: "RADAr Summary"
output: html_document
date: "2023-07-09"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(knitr)
library(car)
library(multcompView)
library(FSA)
library(dunn.test)
```
# Whole-test analysis: RADAr NED
## RADAr NED - First, import and inspect the dataset nedall
### Import RADArNEDall.csv to nedall
The dataset nedall is retrieved from a csv containing all RADAr NED data (currently to Oct 2023), and transformed so that the responses are numeric or NA. In this data set, each row is an item and each column is a test taker. The column and row IDs contain relevant information about each item or test taker. 
```{r retrieve data from csv}
nedall <- read.csv("RADArNEDall.csv", row.names=1,header = TRUE)
```
### Inspect `r nedall`. What values are there?
What are the different values in this set? I can see from below that the values range from 0 to 8. I know that most items are coded 0 or 1 (dichotomous) and the hotspot items are coded 0-8. No values are unexpected. 
```{r what are the unique values in nedall - check}
# Apply unique() function to every column in the data frame
unique_values <- unique(unlist(lapply(nedall, unique)))
# Print the unique values as a vector
print(unique_values)
```
The set should now be purely numeric. Let's test that. The table function shows only one value - TRUE - meaning that all data is now numeric
We can also check that all data is numeric, which is required for the calcluations.
```{r is nedall numeric?}
is_numeric <- sapply(nedall, is.numeric)
table(is_numeric)
```
### What are the numbers of items for each administration of the test?
Let's have a look at the data, starting with the distribution of scores. Firstly, I'd like to see the distribution of the total number of items answered. In the real results, this was likley just an extra point given to the test takers. For analysis purposes, however, I'd like to use the real number of items answered by the candidates. 
The code below double checks the total items answered and their distribution amongst all test takers.
``` {r how many items do students answer? max_score_distribution}
max_score_distribution <- lapply(nedall, function(x) sum(!is.na(x)))
max_score_distribution2 <- unlist(max_score_distribution)
table(max_score_distribution2)

```
It is quite likely that all LETNED19201e test takers have lower test items because the hotspot was not used this administration. 
Two administrations do NOT have a total item number of 115?
```{r which admins had only 114 items?}
odd_one_out_2 <- names(max_score_distribution2)[max_score_distribution2 %in% c(114)]
odd_one_out_3 <- unique(substr(odd_one_out_2, 1, 12))
odd_one_out_3
length(odd_one_out_2)
odd_one_out_2
```
Further inspection of the csv files shows that for the NWINED20221e administration, one of the dictee items was eliminated from the test results, resulting in a count of 114 items (this was possibly due to an unfair element detected after test administration). As for the LETNED19201e, this test did not have a hotspot item, which accounts for the 114 count. 
Now that we have the various max scores sorted, we can compare the admins via their percentage score. 
### Descriptives: Different Administrations
```{r create admins_nedall, vector of all admin names}
nedall_col_names <- colnames(nedall)
admins_nedall <- c(unique(substr(nedall_col_names, 1, 12)))
admins_nedall
```
Now, let's go back to `r max_score_distribution`. This data set contains the max scores for each col in `r nedall`. How can we find out what the max_score_distrbution is for each value in vector admins_nedall, which contains different strings corresponding to the first 12 characters of the col names in nedall.
```{r mean_max_table - how many items @ each admin}
mean_max <- c()
for (admin in admins_nedall) {
  admin_columns <- nedall_col_names[grepl(paste0("^", admin), nedall_col_names)]
  admin_counts <- sum(!is.na(nedall[, admin_columns]))
  admin_mean <- admin_counts / length(admin_columns)
  mean_max <- c(mean_max, admin_mean)
}
mean_max_table <- data.frame(Admin = admins_nedall, Mean_Max = mean_max)
mean_max_table

``` 
The results printed show that the suspected LETNED19201e indeed has 114 (plus one 113) count. The other admins all have whole numbers, indicating that all test takers had that count. 
In the following count, we separate the individual test scores into batches x_scores by admin

```{r group indiv scores by admin}
nedall_col_sums <- colSums(nedall, na.rm=TRUE)
for (admin in admins_nedall) {
  x <- colnames(nedall)[grep(paste0("^", admin), colnames(nedall))]
  y <- nedall_col_sums[names(nedall_col_sums) %in% x]
  assign(paste0(admin, "_scores"), y)
}
scores_admins <- c()
for (admin in admins_nedall) {
  x <- paste0(admin, "_scores")
  scores_admins <- c(scores_admins, x)
}
``` 
Now, let's look at the histograms of all administrations. 
```{r histograms for scores x admins}
for (name in scores_admins) {
  x <- get(name)
  y <- length(x)
  hist(x, main = "" )
  title(paste0(name, "N=", y))
}
``` 
And the boxplots. 
```{r boxplots for scores x admins}
for (name in scores_admins) {
  x <- get(name)
  y <- length(x)
  boxplot(x, main = "")
  title(paste0(name, " N =", y))
}
``` 
### Outliers
Inspection of these boxplots reveal some outliers in some of the sets we can look at. Given the nature of the test: no-stakes, diagnostic, required) it is important to identify outliers so that (later) individual responses can be analyzed to see if the test taker abandoned or skipped sections, or a potentially low score is an accurate representation of ability.  
```{r identifying a list of outlier scores using % scores (scores_admin)}
outliers_all <- c() 
for (name in scores_admins) {
  x <-get(name)
  # Calculate the interquartile range (IQR)
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  iqr <- q3 - q1

# Define the lower and upper bounds for outliers
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr

# Identify outliers
  outliers <- x[x < lower_bound | x > upper_bound]
  outliers_all<- c(outliers_all, outliers)
# Print the outliers

}
sorted_outliers <- sort (outliers_all)
print(sorted_outliers)

```
Some are quite low - e.g. 7 - this is someone who appears to have abandoned the test after the first section.
For now, I'll leave the outliers in, but this section can be returned to later on when running analyses. 
## Functions to navigate nedall with
checkcheck(df) 
Pulls up a (temporary) excel file so you can inspect the data in data frame df.
```{r checkcheck(df)}
checkcheck <- function(df) {
  # Write the data frame to a CSV file
  write.csv(df, file = "checkcheck.csv")
  shell.exec("checkcheck.csv")
}

``` 
Selects only those rows (item IDs) with the search terms in them. Multiple search terms: use c("x", "y") format in the search_terms field.
```{r grabitems(df, search_terms)}
grabitems <- function(df, search_terms) {
  ## this function creates a score csv by grabbing all items with search terms in name
  ## the search terms should be in "" and c("", "" ,"" ) for more than one
  ## Find rows containing the search terms
  search_items <- unique(grep(paste(search_terms, collapse = "|"), 
                              rownames(df), ignore.case = TRUE))
  # Subset the data frame by the search items
  search_df <- df[search_items, ]
  # return data frame
  return(search_df)
}  
``` 
### grabtotalscores(df, search_terms)
```{r grabtotalscores(df, search_terms)}
grabtotalscores <- function(df, search_terms) {
  
  # create an empty data frame to store the results
  sumsdf <-data.frame()
  
  # loop through the search terms
  for (term in search_terms) {
    # grab the TOTAL row for the current search term
    print(term)
    grabdf <- grabitemsplus(df, term)
    grabrow <- grabdf["TOTAL",]
    sumsdf <- rbind(sumsdf, grabrow)
    
  }
  grabdf <- grabitemsplus(df, search_terms[1])
  form <- c(grabdf["FORM", ])
  admin <- c(grabdf["ADMIN", ])
  sumsdf <- rbind(sumsdf, form, admin)
  
  
  rownames(sumsdf) <- c(search_terms, "FORM", "ADMIN")
  sumsdf <- sumsdf[,1:(ncol(sumsdf)-3)]
  
  
  return(sumsdf)
}

``` 
## Constructing allnedplus: adding ADMIN, FORM, NUMBER_ITEMS ,HOTSPOT_MAX, RAW_SCORE, PERCENT_SCORE
The pass rate for NED is 70%. What does this mean for different administrations.
Using the custom function grabitemsplus, additional information is added to nedall as nedallplus
First, we will create nedallplus, a data frame which has additional information. First we will add ADMIN as the first row
```{r  nedallplus - add row ADMIN}

# Create a new row called "ADMIN" with extracted characters from column headings
# Get the column names of the dataframe nedall

# Get the column names of the dataframe nedall
column_names <- colnames(nedall)

# Create a new row with extracted characters and name it "ADMIN"
ADMIN <- substr(column_names, 1, 12)

# Bind the new row to the top of the dataframe
nedallplus1 <- rbind(ADMIN, nedall)
rownames(nedallplus1)[1] <- "ADMIN"
```
Now I want to add another row with item numbers, derrived above with variable `r `mean_max`
```{r add row: Number Items Answered <- nedallplus2}
# Look up values and add to ITEMS vector
ITEMS <- c()
admin_vector <- as.vector(nedallplus1["ADMIN", ])

for (i in 1:ncol(nedallplus1)) {
  admin_value <- admin_vector[i]
  match_row <- match(admin_value, mean_max_table$Admin)
  mean_max_value <- mean_max_table$Mean_Max[match_row]
  ITEMS <- c(ITEMS, mean_max_value)
}
nedallplus2 <- rbind(ITEMS,nedallplus1)
rownames(nedallplus2)[1] <- "NUMBER_ITEMS"
```
Now, some of the administrations use exactly the same form, so the following chunk creates a matrix with admin in col 1 and form in col 2.
```{r create admin to form matrix for lookup purposes (forms_to_admins_vector)}
forms_to_admins_vector <- c("FTRNED21221e","FTRNED(A)",
                            "FTRNED22231e","FTRNED(A)",
                            "LETNED19201e","LETNED(A)",
                            "LETNED20211e","LETNED(B)",
                            "LETNED21221e", "LETNED(B)",
                            "LETNED22231e", "LETNED(B)",
                            "LETNED20212e", "LETNED(C)", 
                            "LETNED21222e", "LETNED(C)",
                            "MANNED20211e", "MANNED(A)",
                            "MANNED21221e", "MANNED(A)",
                            "MANNED21222e", "MANNED(A)",
                            "MANNED22231e", "MANNED(B)",
                            "MEDNED21221e", "MEDNED(A)",
                            "MEDNED22231e", "MEDNED(A)",
                            "NWINED21221e", "NWINED(A)", 
                            "NWINED22231e", "NWINED(A)",
                            "SOWNED22231e", "SOWNED(A)")
forms_admins_matrix <- matrix(forms_to_admins_vector,ncol=2,byrow=TRUE)
print(forms_admins_matrix)
```
Now I want to add another row to nedallplus2 called FORM. 
```{r add row FORM to nedallplus2}
FORM <- c()
admin_vector <- as.vector(nedallplus2["ADMIN", ])
for (i in 1:ncol(nedallplus2)) {
  admin_value <- admin_vector[i]
  match_row <- match(admin_value, forms_admins_matrix[,1])
  form_value <- forms_admins_matrix[,2][match_row]
  FORM <- c(FORM, form_value)
}
nedallplus3 <- rbind(FORM, nedallplus2)
rownames(nedallplus3)[1]<- "FORM"
``` 
The hotspot items are the only non-dichotomous items. Therefore the NUMBER_ITEMS row does not actually give us the maximum score for each form/admin. Cross-referencing all NED hotspot items in use in Cirrus confirms that all have a max of 7. in'to Languages has a way to adjust this to a max of 8, so scores in the data have a max of 8. However, one form - LETNED(A) - does not have a hotspot at all, so the NUMBER_ITEMS is acutally the max. 

In any case, to make this easier, I'll first add the hotspot max score 8 or 0 to the matrix we used earlier for forms, forms_to_admins_vector. 
```{r forms_admins_hotspots_matrix - hotspot points max by admin and form}
hotspot_max <- c(8,8,0,8,8,8,8,8,8,8,8,8,8,8,8,8,8)
forms_admins_hotspots_matrix <- cbind(forms_admins_matrix, hotspot_max)
```
Now I'll use this once again to look up the value and add it to NUMBER_ITEMS in nedallplus3 to create nedallplus4 with row MAX_SCORE
```{r nedallplus4 - new row MAX_SCORE} 
hotspot_max <- c()
admin_vector <- as.vector(nedallplus2["ADMIN", ])
for (i in 1:ncol(nedallplus3)) {
  admin_value <- admin_vector[i]
  match_row <- match(admin_value, forms_admins_hotspots_matrix[,1])
  form_value <- forms_admins_hotspots_matrix[,3][match_row]
  hotspot_max <- c(hotspot_max, form_value)
}

nedallplus4 <- rbind(hotspot_max, nedallplus3)
rownames(nedallplus4)[1]<- "HOTSPOT_MAX"
# calculate row MAX_SCORE
x <- as.vector(as.numeric(nedallplus4["HOTSPOT_MAX",]))
y <- as.vector(as.numeric(nedallplus4["NUMBER_ITEMS",]))
z <- c(x+y)
nedallplus5 <- rbind(z, nedallplus4)
rownames(nedallplus5)[1]<- "MAX_SCORE"
```
Now lets add another row for raw score
```{r RAW_SCORE added}
col_sum <- c()



for (i in 1:ncol(nedallplus5)) {
  x <- nedallplus5[6:nrow(nedallplus5),i]
  y <- as.numeric(x)
  z <- sum(y, na.rm=TRUE)
  col_sum <- c(col_sum, z)
}
nedallplus6 <- rbind(col_sum, nedallplus5)
rownames(nedallplus6)[1]<- "RAW_SCORE"
```
Now that we have the RAW_SCORE, we can calcluate the percentage score
```{r PERCENT_SCORE added}
x <- as.vector(as.numeric(nedallplus6["MAX_SCORE",])) 
y <- as.vector(as.numeric(nedallplus6["RAW_SCORE",]))
z <- c(y/x)
nedallplus7 <- rbind(z, nedallplus6)
rownames(nedallplus7)[1]<- "PERCENT_SCORE"
```
Finally, let's transform the last interation of nedallplus (currently nedallplus7) to a generic name, nedallplus
```{r - TRANSFORM to nedallplus}
nedallplus <- nedallplus7
```
## Descriptive stats for FORM (a better grouping than ADMIN)
`r nedallplus` contains information that will now allow us to inspect different forms for differences. We choose to use forms instead of admins because many admins have only a few tts. 
To do analyses, we need to transform the data so that the col names are the summary data we've added along with item names. 
Let's start by summarizing the Ns for each form using variable $FORM to group a summary:
```{r forms_scores: N, Mean, SD, Min, Max}
forms <- unlist(as.vector(nedallplus["FORM", ]))
scores <- as.numeric(unlist(as.vector(nedallplus["PERCENT_SCORE",])))
form_scores <- data.frame(FORM=forms,SCORES=scores)

summary <- forms_scores %>%
  group_by(FORM) %>%
  summarise(
    count = n(),
    min_score = min(SCORES),
    max_score = max(SCORES),
    mean_score = mean(SCORES),
    sd_score = sd(SCORES)
  )

# Print the result
print(summary)
```

let's look at these in a violin histogram. The red line indicates the average for the overall test, which is stored in variable `r overall_mean`. The chart shows that the individual means for the forms are near but not on the line, indicating that some groups score better on their test form than other groups. Importantly, the test forms are different and the students are siloed, so we cannot yet say that there is an issue of unfairness. Additionally, we do not yet know if these differences are statistically significant. That is the next step. 
```{r For FORMS - Violin Distribution Chart}

# Calculate means by form
overall_mean<- forms_scores %>%
  summarize(mean_score = mean(SCORES))

overall_mean <-unlist(overall_mean)
cat("overall average:", overall_mean)


mean_scores <- forms_scores %>%
  group_by(FORM) %>%
  summarise(mean_score = mean(SCORES))

# Create the violin plot
ggplot(forms_scores, aes(x = FORM, y = SCORES)) +
  geom_violin(fill = "gray", color = "gray") +
  geom_hline(aes(yintercept = mean(SCORES)), linetype = "dashed", color = "red") +
  geom_point(data = mean_scores, aes(x = FORM, y = mean_score), color = "black", size = 3, shape = 19) +
  labs(title = "Distribution of scores in % by form") +
  xlab(NULL) +
  ylab("Scores") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```
## Removing outliers from data set to create nedallplus_no_outliers
The appearence of outliers on the lower end is quite apparent here. It is a good idea now to look back at our outlier list and determine a method by which they can be evaluated for exclusion. The variable `r outliers_all`, calculated earlier, contains the outliers: testtaker and score. Below, a new data frame, `r outliers_df` contains these two rows. But first, let's inspect the outliers raw scores. 
```{r distribution of outliers}
hist(outliers_all)
```
Which should we exclude? Firstly, those who abandon the test. To see this, we need to look at the individual responses to items to see the behaviour. Below, `r outliers_details_collapsed` provides this information. 

```{r exclude outliers? outliers_all}

names_outliers_all <- names(outliers_all)
outliers_df <- data.frame(testtaker=names_outliers_all, score=outliers_all)
outliers_details <- nedallplus_t[outliers_df$testtaker,]
# Collapse the data frame
outliers_details_collapsed <- as.data.frame(t(apply(outliers_details, 1, function(x) {
  x <- na.omit(x)
  c(x, rep(NA, ncol(outliers_details) - length(x)))
})))

#create excel sheet
write.csv(outliers_details_collapsed, "outliers_excel.csv")

```
For this iteration, I saved this file with the date "outliers_excel_20230713", and visually inspected the data by highlighting all 0 values and looking for patterns in which there are large strings of 0s indicating a test was abandoned or a section was skipped accidentally or some other similar situation which would lead to a test score biased by external circumstances. 

Abandonment: The last section of the test is four questions for Drogreden, and the penultimate is TekstArgument, so I started by inspecting score for these and seeing if a test taker may have abandoned. This identified the following test takers as likely having abandoned the test: 
FTRNED21221e8
FTRNED21221e29
LETNED21222e63
MANNED20211e680
MANNED21221e277
MANNED21222e16
MEDNED21221e71

Full 0s for complete sections: long stretches of 0s in an otherwise attempted test may indicates that there was a technical problem (page moved forward unintentionally) or perhaps the test taker themselves chose to not do the section (a risk with no-stakes mandatory tests). In either case, the test results is a less than desirable measure of the construct so these should also be removed from the data set. Special attention is paid to the constructed response items, which test takers may find more desirable to skip than multiple choice questions; a pattern in which all CR items are 0 but the MC responses show a mix of 1 and 0, it seems likely that the test taker did not fully engage with the CR items. The list of test takers who demonstrate this kind of response pattern: 
LETNED19201e3
LETNED19201e40
MANNED21222e3
MANNED22231e516
MANNED22231e70
MEDNED22231e412
NWINED21221e76
SOWNED22231e792
SOWNED22231e793

Finally, outliers are arranged from lowest to highest score. No high scoring outliers are removed from the set. Response patterns of low scoring outliers who were not flagged in the previous two steps were inspected. Two further were flagged for removal: 
MANNED22231e71
MANNED22231e262

The excel file outliers_excel_20230713.xlsx contains the data for this process, with the abandoned tests flagged in orange, the section abandonments flagged on yellow, and suspicious low scorers in blue. The list is preserved in outliers_excel_20230713_list.csv for access by r. 

Let's import that csv file to remove the test takers to create nedallplus_no_outliers. 

```{r nedallplus_no_outliers: removed outliers}
outlier_list <- read.csv("outliers_excel_20230713_list.csv")
outlier_list <- names(outlier_list) # makes a vector
col_nums_outliers <- which(names(nedallplus) %in% outlier_list)
#let's check to make sure the cols match the names in the outliers_list
x<- names(nedallplus[,col_nums_outliers])
setequal(x, outlier_list)
#They match. Now we can use col_nums_outliers to remove outliers and create the new data frame
nedallplus_no_outliers <- nedallplus[, -col_nums_outliers]
# let's double check 
ncol(nedallplus) - ncol(nedallplus_no_outliers)
# the answer, 19, is correct as there were 19 outliers I wished removed. 
```
## Descriptive stats on FORM, outliers removed, (nedallplus_no_outliers)
Now, let's rerun our descriptive stats and violin charts, using the nedallplus_no_outliers data set. 
```{r descriptive table (nedallplus_no_outliers)}
forms2 <- unlist(as.vector(nedallplus_no_outliers["FORM", ]))
scores2 <- as.numeric(unlist(as.vector(nedallplus_no_outliers["PERCENT_SCORE",])))
forms_scores2 <- data.frame(FORM=forms2, SCORES=scores2)

summary <- forms_scores2 %>%
  group_by(FORM) %>%
  summarise(
    count = n(),
    min_score = min(SCORES),
    max_score = max(SCORES),
    mean_score = mean(SCORES),
    sd_score = sd(SCORES)
  )

# Print the result
print(summary)
```
So far so good. The min scores show the influence of having the outliers removed. Now let's reproduce the violin chart.
```{r nedall_no_outliers violin distribution chart}

# Calculate means by form
overall_mean2<- forms_scores2%>%
  summarize(mean_score = mean(SCORES))

overall_mean2 <-unlist(overall_mean2)
cat("overall average:", overall_mean2)


mean_scores2 <- forms_scores2 %>%
  group_by(FORM) %>%
  summarise(mean_score = mean(SCORES))

# Create the violin plot
ggplot(forms_scores2, aes(x = FORM, y = SCORES)) +
  geom_violin(fill = "gray", color = "gray") +
  geom_hline(aes(yintercept = mean(SCORES)), linetype = "dashed", color = "red") +
  geom_point(data = mean_scores, aes(x = FORM, y = mean_score), color = "black", size = 3, shape = 19) +
  labs(title = "Distribution of scores in % by form") +
  xlab(NULL) +
  ylab("Scores") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


```

## Investigating the significance of differences between FORM
Are any of these differences significant? An ANOVA will help us answer this. There are assumptions about the data that must be checked before the ANOVA is performed. Independence: The data is independent. Normality: There is some question about the normality of the distributions of scores as the tend to be left skewed due to a ceiling effect. Homogeneity of Variance: This will need to be checked as well. Random Sampling: In this case, the sample is the population, so this assumption is met. 
ANOVAs are robust against non-normal distribution but we can still check. Kolmogorov–Smirnov is used for larger sample sizes (rather than Shapiro Wilk), but cannot handle data with tied values (which this data has). 
http://www2.psychology.uiowa.edu/faculty/mordkoff/GradStats/part%201/I.07%20normal.pdf
Can we assume normality? perhaps we can inspect skew and kurtosis. If the skewness is negative, this indicates that the distribution is left-skewed. If the kurtosis is greater than 3, this indicates that the distribution has more values in the tails compared to a normal distribution (is flatter).
The table below shows all forms, with N, skew and kurtosis and mean(red) / median(blue) lines. Each form's histogram is shown for reference. While one form has a high kurtosis, there do not appear to be any curves which vary too far from normal distribution (??)

```{r normal distribution of scores in forms_scores2?}

library(moments)
results<- c()
for (group in unique(forms_scores2$FORM)) {
  scores <- forms_scores2$SCORES[forms_scores2$FORM == group]  # Subset scores for the current group
  skew <- skewness(scores)
  kurt <- kurtosis(scores)
  results<- c(results, group, length(scores), skew, kurt) 
  hist(scores, main = group)
  # Calculate the mean and median
  mean_score <- mean(scores)
  median_score <- median(scores)

  # Add the mean line
  abline(v = mean_score, col = "red", lwd = 2)

  # Add the median line
  abline(v = median_score, col = "blue", lwd = 2)
}

results_table <- matrix(results, ncol=4, byrow=TRUE)
colnames(results_table) <- c("Form", "N", "Skewness", "Kurtosis")
results_table
```
Based on these charts, and the robustness of ANOVA against non-normality (citation?), I believe this is ok. As for the assumption Homogeneity of Variance, inspection of boxplots will confirm or not
```{r Homogeneity of Variance?}
# Create side-by-side boxplots
boxplot(SCORES ~ FORM, data = forms_scores2, 
        main = "Homogeneity of Variance",
        xlab = "Group", ylab = "SCORES")

# Add a reference line at the overall median
overall_median <- median(forms_scores2$SCORES)
abline(h = overall_median, col = "red", lwd = 2)


```
It appears as if the spreads are quite similar, and there is no violation of the assumption. 
However, we can test with 
``` {r Testing Homogeneity of Variance with Levenes test:}
leveneTest(SCORES ~ FORM, data = forms_scores2)
```
The p-value is very small (6.279e-06), indicating strong evidence against the null hypothesis. Therefore, you can conclude that there is evidence of heterogeneity of variances among the groups based on Levene's test.
Let's investigate now the magnitude of Heterogeneity: The larger the differences in variances among groups, the more serious the violation of the assumption. Large differences in variances can affect the precision of the estimated group means and can lead to biased estimates of the treatment effects.

```{r Magnitude of Heterogeneity investigated visually}
group_variances <- tapply(forms_scores2$SCORES, forms_scores2$FORM, var)
group_sd <- tapply(forms_scores2$SCORES, forms_scores2$FORM, sd)

# Density plot
library(ggplot2)
ggplot(forms_scores2, aes(x = SCORES, fill = FORM)) +
  geom_density(alpha = 0.5) +
  theme_minimal()

# Boxplot
ggplot(forms_scores2, aes(x = FORM, y = SCORES)) +
  geom_boxplot() +
  theme_minimal()

```
From the density plot, it seems as if one form LETNED(B) has a different variance than the other forms. ANOVA is fairly robust in terms of the error rate when sample sizes are equal. However, when sample sizes are unequal, ANOVA is not robust to violations of homogeneity of variance. As the sample size of the groups FORM are quite varied, this heterogeneity of variance may be an issue. 
Just for fun, let's run the ANOVA anyhow. 
```{r }
# Run one-way ANOVA
anova_result <- aov(SCORES ~ FORM, data = forms_scores2)

# Print the ANOVA table
summary(anova_result)


```
The p-value is extremely small (<2e-16), indicating strong evidence against the null hypothesis. Therefore, you can conclude that there are significant differences among the groups based on the ANOVA results.

Now, let's do some post-hoc tests to see where these differences are. 
```{r}
posthoc_tukey <- TukeyHSD(anova_result)
posthoc_tukey
```
Let's inspect this graphically
```{r posthoc_tukey represented in boxplots}
# Generate the letter-value plot with group names
plot(posthoc_tukey , las=1 , col="brown")



```
The x-axis here represents difference in mean % score, so 0.05 represents a 5 point difference in a score of 100. 
Let's look only at the significant differences, arranged by difference in mean between groups. 
```{r - only significant, arranged by diff in mean}
x <- as.data.frame(posthoc_tukey$FORM)

# Add a new column 'is_sig' based on 'p.adj' values
x$sig <- x$'p adj' <= 0.05

# Print the updated data frame

x <- x[order(x$diff), ]
y <- x[x$sig, ]
z <- arrange(y, diff)
z
```
Let's have a look at the singificant difference in means in a graphical form:
```{r significant diff min means graphed}

z <- z %>% 
  mutate(row_order = row_number())

# Sort the data frame based on the 'diff' column
z <- arrange(z, diff)

plot <- ggplot(z, aes(x = diff, y = row_order)) +
  geom_errorbarh(aes(xmin = lwr, xmax = upr), height = 0.2) +
  geom_point(size = 2) +
  geom_text(aes(label = rownames(z)), vjust = -.8,  size = 2) +  # Add row names as text
  labs(title = "Significant Mean Differences between Forms",
       x = "Mean Difference",
       y = "Row")
  

plot <- plot + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
                     axis.title.y = element_blank(),
                     axis.text.y = element_blank(),
                     axis.ticks.y = element_blank())

# Display the plot
print(plot)
```
It appears as if there are some significant differences in means between groups, which largely fall etween 2.5 points and 5 points on a 100 point test. The difference between the two MANNED forms, however, shows a difference in over 7 points, and these two forms show significant differences with many other forms as well. 

As we had some earlier concerns about the use of the ANOVA as the data violates some of its assumptions, let's explore a non-parametric alternative.

The Kruskal-Wallis test is a non-parametric statistical test used to determine if there are any statistically significant differences between three or more independent groups. It is an extension of the Mann-Whitney U test, which is used to compare two independent groups. The Kruskal-Wallis test is suitable when the data do not meet the assumptions of parametric tests, such as the assumption of normality or equal variances.

The Kruskal-Wallis test is a hypothesis test that compares group medians, not means. If the test suggests significant differences between the groups, further post-hoc tests (e.g., Dunn's test) can be conducted to identify which specific groups differ significantly from each other.

First, let's inspect the means again and see the differences. 

```{r Kruskal-Wallis test : Descriptive : means}

medians <- aggregate(SCORES ~ FORM, data = forms_scores2, FUN = median)
medians_ordered <- medians %>%
  arrange(desc(SCORES))


# Print the ordered medians
print("The median scores of all forms in descending order")
print(medians_ordered)

```
The difference is 0.08 (8 points), and this spread and the figures match very closely to mean scores. 
```{r}

# Perform the Kruskal-Wallis test
kruskal_result <- kruskal.test(SCORES ~ FORM, data = forms_scores2)
```
The Kruskal-Wallis test revealed a significant difference among the groups based on the scores (chi-squared = 579.33, df = 8, p < 0.001).
A post-hoc test is warranted
```{r Dunn post-hoc : dunn_result (bonferroni)}
# Perform Dunn's test as a post-hoc test
dunn_result <- dunn.test(forms_scores2$SCORES, g = forms_scores2$FORM, method = "bonferroni")

# Display the post-hoc test results
print(dunn_result)
```
Let's compare the values for the Tukey (means) and the Dunn (median) tests. First let's see which pairs are significant and what the p values are
```{r }
# create df for dunn results:
comparisons <- as.vector(dunn_result$comparisons)
diff_median <- as.vector(dunn_result$P.adjusted)
significant <- as.vector(dunn_result$P.adjusted <= 0.05)

dunn_summary_filtered <- dunn_summary %>% filter(sig == TRUE)
dunn_summary_filtered
```
Now, let's compare the Dunn groups with the Tukey groups
```{r} 
dunn_pairs <- dunn_summary_filtered$comparisons
tukey_pairs <- rownames(z)
dunn_pairs <- gsub(" ", "", dunn_pairs)
dunn_pairs <- sub("(.*)-(.*)", "\\2-\\1", dunn_pairs)
dunn_pairs <- sort(dunn_pairs)
tukey_pairs <- sort(tukey_pairs)
# Assuming you have vectors named dunn_pairs and tukey_pairs

# Compare elements and identify those present in dunn_pairs but not in tukey_pairs
in_dunn_not_in_tukey <- setdiff(dunn_pairs, tukey_pairs)

# Compare elements and identify those present in tukey_pairs but not in dunn_pairs
in_tukey_not_in_dunn <- setdiff(tukey_pairs, dunn_pairs)

# Print the results
cat("Length of dunn_pairs: ", length(dunn_pairs), "\n")
cat("Length of tukey_pairs: ", length(tukey_pairs), "\n")
cat("Elements in dunn_pairs but not in tukey_pairs:", in_dunn_not_in_tukey, "\n")
cat("Elements in tukey_pairs but not in dunn_pairs:", in_tukey_not_in_dunn, "\n")
``` 
So, the significant differences matches for both ANOVA (means) and Kruskal-Wallis (medians). I am comfortable assuming that these 19 pairs (and only these 19 pairs, out of 36 pairs total) are significantly different: 
```{r list of significantly different groups}
dunn_pairs 
```
Because the two tests (ANOVA and Kruskal-Wallis) have such similar results, I feel comfortable accepting the Tukey Test results as indications of the difference in means 

What are the pass/fail rates for each form (70% for NED)?
```{r}
#create empty vector passfail()
#using nedallpluslong
#$FORM and $PERCENT
# for loop for form in allforms
# gather all $PERCENT for $FORM
# count number >= 70, <- pass
# total - pass = <- fail 
# pass/total <- perc_pass
# pass FORM, total, pass, fail, perc_pass to vector passfail
# create matrix passfail_matrix from vector passfail using form as row names and designate col names manually
# print(passfail_matrix)
```
Are there significant differences in the expected vs observed pass/fall numbers
```{r}
# use passfail_matrix for analysis
# use chisquare to determine significant differences
# post hoc to identify where
```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```













